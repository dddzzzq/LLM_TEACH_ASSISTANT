### **实验设计与评估指标详细汇报**

#### **2. 实验数据集介绍**

为了确保评估的全面性，本次实验选取了涵盖源代码和中文自然语言处理的三个代表性数据集：

* **SOCO (Java) 代码抄袭数据集**:
    * 这是一个开源的Java源代码重用数据集 。
    * 实验中使用了包含代码对及其标签（1为抄袭，0为原创）的CSV文件版本，用于评估模型在检测**源代码抄袭**方面的性能 。

* **LCQMC (大规模中文问题匹配语料库)**:
    * 由哈尔滨工业大学构建的中文问题匹配数据集，用于判断两个问题在语义上是否等价 。
    * 实验中用于评估模型在**中文自然语言**场景下识别**语义相似性**的能力，这与检测释义、改写等高级抄袭手段直接相关 。

* **PAWS-X (中文)**:
    * 一个释义识别数据集，其特点是包含了大量词汇重叠度高但语义可能不同（或相同）的句子对，具有很高的迷惑性 。
    * 该数据集用于测试模型在处理**复杂和细微语义差异**时的鲁棒性，是评估高级文本抄袭检测能力的理想选择 。

#### **3. 实验组设计与介绍**

实验我共设置了六个实验组，包括三个基线模型、一个对比模型、本系统方案以及一个消融实验组。

* **3.1 基线模型1：TF-IDF**
    * **方法介绍**: TF-IDF（词频-逆文档频率）是一种经典的数值统计方法，通过计算词语在文档中的频率及其在整个语料库中的普遍程度来评估词的重要性 。该方法将文本转换为向量，并使用余弦相似度进行比较 。
    * **实验目的**: 作为最传统的基线，用于衡量后续更复杂模型的性能提升幅度 。

* **3.2 基线模型2：单一小模型 (UniXcoder)**
    * **方法介绍**: UniXcoder是一个强大的预训练Transformer模型，能够支持多种编程语言，并将源代码文件结构化为高维嵌入向量 。通过计算这些向量的余弦相似度来判断代码的相似性 。
    * **实验目的**: 评估一个专用于代码的预训练小模型在无大语言模型介入下的基准性能 。

* **3.3 基线模型3：单一小模型 (BERT)**
    * **方法介绍**: BERT（来自Transformer的双向编码器表示）是一种基于Transformer的双向编码模型，能够深度捕捉文本的双向语境信息，生成富含语义的句子向量 。
    * **实验目的**: 评估一个经典的预训练语言模型在处理文本相似性任务时的基准性能 。

* **3.4 对比实验：双模型（无LLM）**
    * **方法介绍**: 该模型模拟了一个多特征融合的检测系统，能够区分文本和代码内容，并分别进行相似度计算，最终合并结果 。这反映了现实场景中作业常包含多种内容形式的需求 。
    * **实验目的**: 验证相比单一模型，对内容进行分离处理（文本/代码）是否能带来性能提升。

* **3.5 本系统方案：两阶段混合模型 (Our System w/ LLM)**
    * **方法介绍**: 这是我们提出的核心方案。它采用两阶段检测流程：
        1.  **第一阶段 (初筛)**: 使用双模型（UniXcoder和BERT）对文本和代码进行分离和快速初筛，找出高度可疑的作业对 。
        2.  **第二阶段 (LLM精判)**: 将初筛出的可疑作业对提交给大语言模型（如DeepSeek）进行深度语义分析和逻辑判断，得出最终结论 。
    * **实验目的**: 验证结合小模型的效率和LLM的深度分析能力，是否能在保证高准确率的同时，有效控制LLM的调用成本。

* **3.6 消融实验：无内容分离混合模型 (Ablation w/o Separation)**
    * **方法介绍**: 该模型作为“本系统方案”的消融版本，移除了内容分离步骤。它使用单一模型（如UniXcoder）对整个提交内容（文本代码混合）进行初筛，然后将整个内容提交给LLM进行精判 。
    * **实验目的**: 通过与核心方案对比，验证**内容分离**这一步骤对于提升混合模型性能的重要性。

#### **4. 评估指标介绍**

* **基础概念**:
    * **TP (True Positive)**: 成功检测出抄袭。
    * **FP (False Positive)**: 将原创误判为抄袭（误报）。
    * **FN (False Negative)**: 将抄袭误判为原创（漏报）。
    * **TN (True Negative)**: 成功识别为原创。

* **核心评估指标**:
    * **`accuracy` (准确率)**: 模型正确预测（TP+TN）占总样本的比例 。在数据不平衡时可能产生误导。
    * **`precision_plag` (抄袭类精确率)**: 模型预测为抄袭的样本中，真正是抄袭的比例 `(TP / (TP + FP))` 。
    * **`recall_plag` (抄袭类召回率)**: 所有真实抄袭的样本中，被模型成功找出的比例 `(TP / (TP + FN))` 。
    * **`f1_plag` (抄袭类F1值)**: 精确率和召回率的调和平均数，综合反映模型在抄袭类上的表现 。
    * **`recall_macro` (总体召回率 / 宏召回率)**: `(recall_plag + recall_orig) / 2` 。它平等对待“抄袭”和“原创”两个类别，反映模型在所有类别上的平均查全能力。
    * **`f1_macro` (总体F1值 / 宏F1分数)**: `(f1_plag + f1_orig) / 2` 。这是在不平衡数据集上评估模型**整体稳健性的关键指标**，一个高的宏F1值说明模型在多数类（原创）和少数类（抄袭）上均表现出色。
    * **`throughtput`（吞吐量）**：`总处理对数/模型运行时间`直观反映模型的处理速度，每秒处理对数。指标越高说明模型的计算效率越高。可以清晰展示tfidf这种轻量级方法与基于深度学习模型在速度上的巨大差异。
    * **`benefit-cost ratio`（效益-成本比）**：`F1分数（抄袭类）/模型运行时间`将效果（准确率/F1分数）和成本（时间）联系起来，量化了“性价比”。
    * 可视化分析：效率前沿图，X轴和Y轴分别表示模型运行时间和F1分数